{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability operations\n",
    "\n",
    "1. Basic Probability Operations:\n",
    "   - Empirical probability calculation\n",
    "   - Joint probability\n",
    "   - Conditional probability\n",
    "   - Bayes' theorem\n",
    "\n",
    "2. Probability Distributions:\n",
    "   - Normal distribution\n",
    "   - Uniform distribution\n",
    "   - Bernoulli and Binomial\n",
    "   - Poisson distribution\n",
    "   - Categorical and Multivariate\n",
    "\n",
    "3. Advanced Probability Operations:\n",
    "   - Entropy calculation\n",
    "   - KL divergence\n",
    "   - Cross entropy\n",
    "   - Mutual information\n",
    "\n",
    "4. Sampling Operations:\n",
    "   - Random sampling\n",
    "   - Importance sampling\n",
    "   - Rejection sampling\n",
    "\n",
    "5. Probabilistic Models:\n",
    "   - Gaussian Mixture Models\n",
    "   - Hidden Markov Models\n",
    "\n",
    "6. Probability Metrics and Tests:\n",
    "   - Confidence intervals\n",
    "   - Likelihood ratio tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as dist\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Basic Probability Operations =====\n",
    "class BasicProbability:\n",
    "    @staticmethod\n",
    "    def calculate_empirical_probability(events, total_samples):\n",
    "        \"\"\"Calculate empirical probability of events\"\"\"\n",
    "        return events.float() / total_samples\n",
    "\n",
    "    @staticmethod\n",
    "    def joint_probability(x, y):\n",
    "        \"\"\"Calculate joint probability P(X,Y)\"\"\"\n",
    "        total = len(x)\n",
    "        unique_pairs = torch.stack([x, y]).T\n",
    "        joint_counts = torch.unique(unique_pairs, dim=0, return_counts=True)[1]\n",
    "        return joint_counts.float() / total\n",
    "\n",
    "    @staticmethod\n",
    "    def conditional_probability(x, y):\n",
    "        \"\"\"Calculate conditional probability P(X|Y)\"\"\"\n",
    "        total_y = torch.sum(y)\n",
    "        joint = torch.sum((x == 1) & (y == 1))\n",
    "        return joint.float() / total_y\n",
    "\n",
    "    @staticmethod\n",
    "    def bayes_theorem(prior, likelihood, evidence):\n",
    "        \"\"\"\n",
    "        Implement Bayes' Theorem: P(A|B) = P(B|A) * P(A) / P(B)\n",
    "        \"\"\"\n",
    "        return (likelihood * prior) / evidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Probability Distributions =====\n",
    "class ProbabilityDistributions:\n",
    "    def __init__(self):\n",
    "        self.distributions = {}\n",
    "        \n",
    "    def normal_distribution(self, mu, sigma):\n",
    "        \"\"\"Create normal distribution\"\"\"\n",
    "        return dist.Normal(mu, sigma)\n",
    "    \n",
    "    def uniform_distribution(self, low, high):\n",
    "        \"\"\"Create uniform distribution\"\"\"\n",
    "        return dist.Uniform(low, high)\n",
    "    \n",
    "    def bernoulli_distribution(self, probs):\n",
    "        \"\"\"Create Bernoulli distribution\"\"\"\n",
    "        return dist.Bernoulli(probs)\n",
    "    \n",
    "    def binomial_distribution(self, n, p):\n",
    "        \"\"\"Create binomial distribution\"\"\"\n",
    "        return dist.Binomial(n, p)\n",
    "    \n",
    "    def poisson_distribution(self, rate):\n",
    "        \"\"\"Create Poisson distribution\"\"\"\n",
    "        return dist.Poisson(rate)\n",
    "    \n",
    "    def categorical_distribution(self, probs):\n",
    "        \"\"\"Create categorical distribution\"\"\"\n",
    "        return dist.Categorical(probs)\n",
    "    \n",
    "    def multivariate_normal(self, mean, covariance):\n",
    "        \"\"\"Create multivariate normal distribution\"\"\"\n",
    "        return dist.MultivariateNormal(mean, covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Advanced Probability Operations =====\n",
    "class AdvancedProbability:\n",
    "    @staticmethod\n",
    "    def entropy(probs):\n",
    "        \"\"\"Calculate Shannon entropy\"\"\"\n",
    "        return -torch.sum(probs * torch.log2(probs + 1e-10))\n",
    "    \n",
    "    @staticmethod\n",
    "    def kl_divergence(p, q):\n",
    "        \"\"\"Calculate Kullback-Leibler divergence\"\"\"\n",
    "        return torch.sum(p * (torch.log(p + 1e-10) - torch.log(q + 1e-10)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_entropy(p, q):\n",
    "        \"\"\"Calculate cross entropy\"\"\"\n",
    "        return -torch.sum(p * torch.log(q + 1e-10))\n",
    "    \n",
    "    @staticmethod\n",
    "    def mutual_information(joint_probs, marginal_x, marginal_y):\n",
    "        \"\"\"Calculate mutual information\"\"\"\n",
    "        log_ratio = torch.log(joint_probs / (marginal_x.unsqueeze(1) * marginal_y.unsqueeze(0)) + 1e-10)\n",
    "        return torch.sum(joint_probs * log_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Sampling Operations =====\n",
    "class SamplingOperations:\n",
    "    @staticmethod\n",
    "    def random_sample(distribution, sample_size):\n",
    "        \"\"\"Generate random samples from distribution\"\"\"\n",
    "        return distribution.sample((sample_size,))\n",
    "    \n",
    "    @staticmethod\n",
    "    def importance_sampling(target_dist, proposal_dist, num_samples):\n",
    "        \"\"\"Perform importance sampling\"\"\"\n",
    "        samples = proposal_dist.sample((num_samples,))\n",
    "        log_weights = target_dist.log_prob(samples) - proposal_dist.log_prob(samples)\n",
    "        weights = torch.exp(log_weights)\n",
    "        normalized_weights = weights / weights.sum()\n",
    "        return samples, normalized_weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def rejection_sampling(target_dist, proposal_dist, M, num_samples):\n",
    "        \"\"\"Perform rejection sampling\"\"\"\n",
    "        accepted_samples = []\n",
    "        while len(accepted_samples) < num_samples:\n",
    "            proposed = proposal_dist.sample()\n",
    "            ratio = target_dist.log_prob(proposed).exp() / (M * proposal_dist.log_prob(proposed).exp())\n",
    "            if torch.rand(1) < ratio:\n",
    "                accepted_samples.append(proposed)\n",
    "        return torch.stack(accepted_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Probabilistic Models =====\n",
    "class ProbabilisticModels:\n",
    "    @staticmethod\n",
    "    def gaussian_mixture_model(x, means, covs, weights):\n",
    "        \"\"\"\n",
    "        Evaluate Gaussian Mixture Model probability\n",
    "        \"\"\"\n",
    "        num_components = len(means)\n",
    "        probs = torch.zeros(len(x))\n",
    "        \n",
    "        for i in range(num_components):\n",
    "            distribution = dist.MultivariateNormal(means[i], covs[i])\n",
    "            probs += weights[i] * torch.exp(distribution.log_prob(x))\n",
    "            \n",
    "        return probs\n",
    "    \n",
    "    @staticmethod\n",
    "    def hidden_markov_model(observations, transition_matrix, emission_matrix, initial_probs):\n",
    "        \"\"\"\n",
    "        Forward algorithm for Hidden Markov Model\n",
    "        \"\"\"\n",
    "        num_states = len(initial_probs)\n",
    "        num_observations = len(observations)\n",
    "        \n",
    "        # Forward matrix\n",
    "        forward = torch.zeros(num_observations, num_states)\n",
    "        \n",
    "        # Initialize first observation\n",
    "        forward[0] = initial_probs * emission_matrix[:, observations[0]]\n",
    "        \n",
    "        # Forward pass\n",
    "        for t in range(1, num_observations):\n",
    "            for s in range(num_states):\n",
    "                forward[t, s] = torch.sum(forward[t-1] * transition_matrix[:, s]) * \\\n",
    "                              emission_matrix[s, observations[t]]\n",
    "                              \n",
    "        return forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Probability Metrics and Tests =====\n",
    "class ProbabilityMetrics:\n",
    "    @staticmethod\n",
    "    def compute_confidence_interval(data, confidence=0.95):\n",
    "        \"\"\"\n",
    "        Compute confidence interval for mean\n",
    "        \"\"\"\n",
    "        n = len(data)\n",
    "        mean = torch.mean(data)\n",
    "        std_err = torch.std(data, unbiased=True) / torch.sqrt(torch.tensor(n).float())\n",
    "        \n",
    "        # Using normal approximation\n",
    "        z_score = torch.tensor(1.96)  # 95% confidence\n",
    "        margin_error = z_score * std_err\n",
    "        \n",
    "        return {\n",
    "            'mean': mean,\n",
    "            'confidence_interval': (mean - margin_error, mean + margin_error),\n",
    "            'confidence_level': confidence\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def likelihood_ratio_test(null_ll, alt_ll, df):\n",
    "        \"\"\"\n",
    "        Perform likelihood ratio test\n",
    "        \"\"\"\n",
    "        lr_statistic = -2 * (null_ll - alt_ll)\n",
    "        # Chi-square test statistic\n",
    "        return {\n",
    "            'test_statistic': lr_statistic,\n",
    "            'degrees_of_freedom': df\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Example Usage =====\n",
    "def demonstrate_probability_operations():\n",
    "    # Basic probability example\n",
    "    events = torch.tensor([1, 1, 0, 1, 0])\n",
    "    prob = BasicProbability.calculate_empirical_probability(events.sum(), len(events))\n",
    "    print(f\"Empirical probability: {prob}\")\n",
    "    \n",
    "    # Distribution example\n",
    "    dist_ops = ProbabilityDistributions()\n",
    "    normal_dist = dist_ops.normal_distribution(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "    samples = normal_dist.sample((1000,))\n",
    "    \n",
    "    # Advanced probability example\n",
    "    probs = torch.tensor([0.3, 0.4, 0.3])\n",
    "    entropy = AdvancedProbability.entropy(probs)\n",
    "    print(f\"Entropy: {entropy}\")\n",
    "    \n",
    "    # Sampling example\n",
    "    proposal_dist = dist.Normal(0, 1)\n",
    "    target_dist = dist.Normal(1, 0.5)\n",
    "    samples, weights = SamplingOperations.importance_sampling(target_dist, proposal_dist, 1000)\n",
    "    \n",
    "    return {\n",
    "        'empirical_prob': prob,\n",
    "        'samples': samples,\n",
    "        'entropy': entropy,\n",
    "        'importance_weights': weights\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
