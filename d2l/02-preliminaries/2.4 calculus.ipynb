{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus Operations\n",
    "\n",
    "**Gradient Operations:**\n",
    "\n",
    "- Basic gradient computation\n",
    "- Vector gradients\n",
    "- Jacobian computation\n",
    "- Hessian computation\n",
    "\n",
    "\n",
    "**Integration Methods:** \n",
    "\n",
    "- Trapezoidal rule\n",
    "- Simpson's rule\n",
    "- Numerical integration techniques\n",
    "\n",
    "\n",
    "**Differential Equations:**\n",
    "\n",
    "- Euler method\n",
    "- 4th order Runge-Kutta method\n",
    "- ODE solvers\n",
    "\n",
    "\n",
    "**Advanced Gradient Operations:**\n",
    "\n",
    "- Vector-Jacobian products\n",
    "- Jacobian-vector products\n",
    "- Hessian-vector products\n",
    "- Parameter gradients\n",
    "\n",
    "\n",
    "**Optimization-Related Calculus:**\n",
    "\n",
    "- Gradient descent\n",
    "- Optimization algorithms\n",
    "- Loss function derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ===== Gradients and Derivatives ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic gradient computation\n",
    "def basic_gradients():\n",
    "    # Create tensor with gradient tracking\n",
    "    x = torch.tensor([2.0], requires_grad=True)\n",
    "    y = x ** 2 + 2*x + 1\n",
    "    \n",
    "    # Compute gradient dy/dx\n",
    "    y.backward()\n",
    "    gradient = x.grad\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gradient for vector-valued functions\n",
    "def vector_gradients():\n",
    "    x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "    y = torch.sum(x ** 2)\n",
    "    \n",
    "    # Compute gradient of y with respect to x\n",
    "    y.backward()\n",
    "    gradient = x.grad\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Jacobian computation\n",
    "def compute_jacobian():\n",
    "    x = torch.randn(3, requires_grad=True)\n",
    "    y = torch.tensor([x[0] ** 2, x[1] ** 3, x[2] ** 4])\n",
    "    \n",
    "    # Compute Jacobian manually\n",
    "    jacobian = torch.stack([autograd.grad(yi, x, retain_graph=True)[0] \n",
    "                          for yi in y])\n",
    "    return jacobian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hessian computation\n",
    "def compute_hessian():\n",
    "    x = torch.randn(3, requires_grad=True)\n",
    "    y = torch.sum(x ** 2)\n",
    "    \n",
    "    # First derivatives\n",
    "    grad = autograd.grad(y, x, create_graph=True)[0]\n",
    "    \n",
    "    # Second derivatives (Hessian)\n",
    "    hessian = torch.stack([autograd.grad(g, x, retain_graph=True)[0]\n",
    "                          for g in grad])\n",
    "    return hessian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ===== Integration Methods =====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_integration():\n",
    "    # Trapezoidal rule\n",
    "    def trapezoid(y, dx):\n",
    "        return dx * (torch.sum(y[1:-1]) + (y[0] + y[-1])/2)\n",
    "    \n",
    "    # Simpson's rule\n",
    "    def simpson(y, dx):\n",
    "        return dx/3 * (y[0] + y[-1] + \n",
    "                      4*torch.sum(y[1:-1:2]) +\n",
    "                      2*torch.sum(y[2:-1:2]))\n",
    "    \n",
    "    # Example usage\n",
    "    x = torch.linspace(0, 1, 1001)\n",
    "    y = torch.sin(x)\n",
    "    dx = x[1] - x[0]\n",
    "    \n",
    "    trap_result = trapezoid(y, dx)\n",
    "    simp_result = simpson(y[::2], dx*2)\n",
    "    \n",
    "    return trap_result, simp_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ===== Differential Equations =====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_method(f, y0, t_span, dt):\n",
    "    \"\"\"Simple Euler method for ODEs\"\"\"\n",
    "    t = torch.arange(t_span[0], t_span[1], dt)\n",
    "    y = torch.zeros(len(t))\n",
    "    y[0] = y0\n",
    "    \n",
    "    for i in range(len(t)-1):\n",
    "        y[i+1] = y[i] + dt * f(t[i], y[i])\n",
    "    \n",
    "    return t, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def runge_kutta_4(f, y0, t_span, dt):\n",
    "    \"\"\"4th order Runge-Kutta method\"\"\"\n",
    "    t = torch.arange(t_span[0], t_span[1], dt)\n",
    "    y = torch.zeros(len(t))\n",
    "    y[0] = y0\n",
    "    \n",
    "    for i in range(len(t)-1):\n",
    "        k1 = f(t[i], y[i])\n",
    "        k2 = f(t[i] + dt/2, y[i] + dt*k1/2)\n",
    "        k3 = f(t[i] + dt/2, y[i] + dt*k2/2)\n",
    "        k4 = f(t[i] + dt, y[i] + dt*k3)\n",
    "        \n",
    "        y[i+1] = y[i] + (dt/6) * (k1 + 2*k2 + 2*k3 + k4)\n",
    "    \n",
    "    return t, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ===== Advanced Gradient Operations =====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedGradients:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient_with_respect_to_params(model, loss):\n",
    "        \"\"\"Compute gradients with respect to model parameters\"\"\"\n",
    "        return torch.autograd.grad(loss, model.parameters())\n",
    "    \n",
    "    @staticmethod\n",
    "    def vector_jacobian_product(vector, jacobian):\n",
    "        \"\"\"Compute vector-Jacobian product\"\"\"\n",
    "        return torch.sum(vector * jacobian)\n",
    "    \n",
    "    @staticmethod\n",
    "    def jacobian_vector_product(jacobian, vector):\n",
    "        \"\"\"Compute Jacobian-vector product\"\"\"\n",
    "        return torch.matmul(jacobian, vector)\n",
    "    \n",
    "    @staticmethod\n",
    "    def hessian_vector_product(grad_output, output, input):\n",
    "        \"\"\"Compute Hessian-vector product\"\"\"\n",
    "        return autograd.grad(grad_output, input, retain_graph=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===== Optimization-Related Calculus ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization_calculus():\n",
    "    # Create a simple optimization problem\n",
    "    x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "    optimizer = torch.optim.SGD([x], lr=0.1)\n",
    "    \n",
    "    def objective(x):\n",
    "        return torch.sum(x**2)\n",
    "    \n",
    "    # Gradient descent step\n",
    "    loss = objective(x)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return x.data, x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Example Usage =====\n",
    "def demonstrate_calculus():\n",
    "    # Basic gradient example\n",
    "    x = torch.tensor([2.0], requires_grad=True)\n",
    "    y = x**3 + 2*x\n",
    "    y.backward()\n",
    "    print(f\"Gradient of x^3 + 2x at x=2: {x.grad}\")\n",
    "    \n",
    "    # Integration example\n",
    "    x = torch.linspace(0, 2*torch.pi, 1000)\n",
    "    y = torch.sin(x)\n",
    "    trap_result, simp_result = numerical_integration()\n",
    "    print(f\"Integration results - Trapezoid: {trap_result}, Simpson: {simp_result}\")\n",
    "    \n",
    "    # ODE example\n",
    "    def f(t, y):\n",
    "        return -y  # Simple decay equation dy/dt = -y\n",
    "    \n",
    "    t, y = runge_kutta_4(f, torch.tensor(1.0), (0, 5), 0.01)\n",
    "    print(f\"ODE solution at t=5: {y[-1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
