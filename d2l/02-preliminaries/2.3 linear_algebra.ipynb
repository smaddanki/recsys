{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Properties\n",
    "\n",
    "Definition:\n",
    "\n",
    "1. Vector Space Properties:\n",
    "- Closure under addition: u + v ∈ V\n",
    "- Closure under scalar multiplication: cv ∈ V\n",
    "- Associativity: (u + v) + w = u + (v + w)\n",
    "- Commutativity: u + v = v + u\n",
    "- Zero vector existence: 0 + v = v\n",
    "- Inverse existence: v + (-v) = 0\n",
    "\n",
    "2. Matrix Properties:\n",
    "- Associativity: (AB)C = A(BC)\n",
    "- Distributivity: A(B + C) = AB + AC\n",
    "- Non-commutativity: AB ≠ BA generally\n",
    "- Transpose: (AB)^T = B^TA^T\n",
    "- Inverse: (AB)⁻¹ = B⁻¹A⁻¹\n",
    "\n",
    "3. Determinant Properties:\n",
    "- det(AB) = det(A)det(B)\n",
    "- det(A^T) = det(A)\n",
    "- det(A⁻¹) = 1/det(A)\n",
    "- det(cA) = cⁿdet(A)\n",
    "\n",
    "4. Trace Properties:\n",
    "- tr(A + B) = tr(A) + tr(B)\n",
    "- tr(AB) = tr(BA)\n",
    "- tr(cA) = c tr(A)\n",
    "- tr(A^T) = tr(A)\n",
    "\n",
    "5. Rank Properties:\n",
    "- rank(AB) ≤ min(rank(A), rank(B))\n",
    "- rank(A) = rank(A^T)\n",
    "- rank(A) ≤ min(m,n) for A ∈ ℝᵐˣⁿ\n",
    "- rank(A) = number of non-zero singular values\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Algorithm design\n",
    "- Optimization problems\n",
    "- Dimensionality analysis\n",
    "- Matrix factorizations\n",
    "- Feature engineering\n",
    "- Model validation\n",
    "- System analysis\n",
    "- Numerical stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Operations\n",
    "\n",
    "**Basic Matrix Operations:**\n",
    "\n",
    "- Matrix multiplication (mm, matmul, @)\n",
    "- Vector operations (mv, dot, cross)\n",
    "- Element-wise operations\n",
    "\n",
    "\n",
    "**Matrix Properties:**\n",
    "\n",
    "- Determinant and rank\n",
    "- Trace and diagonal\n",
    "- Various matrix norms\n",
    "\n",
    "\n",
    "**Matrix Decompositions:**\n",
    "\n",
    "- SVD\n",
    "- Eigendecomposition\n",
    "- LU, QR, and Cholesky decompositions\n",
    "\n",
    "\n",
    "**System Solving:**\n",
    "\n",
    "- Linear system solutions\n",
    "- Matrix inverse and pseudoinverse\n",
    "\n",
    "\n",
    "**Advanced Operations:**\n",
    "\n",
    "- Matrix functions (exp, power)\n",
    "- Special matrix operations\n",
    "- Batch operations\n",
    "\n",
    "\n",
    "**Special Matrices:**\n",
    "\n",
    "- Creation of identity, zero, random matrices\n",
    "- Specialized matrix types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{A} = \n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3\\\\\n",
    "a & b & c\n",
    "\\end{pmatrix}\n",
    "$$ \n",
    "\n",
    "$$ \\text{B} = \n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3\\\\\n",
    "a & b & c\n",
    "\\end{bmatrix}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4],\n",
       "        [10, 20, 30, 40]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1,2,3,4], [10,20,30, 40]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21, 36, 90,  4],\n",
       "        [96, 15, 14, 59],\n",
       "        [95, 62, 55, 94],\n",
       "        [90, 39, 49, 10]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.randint(low=1,high=100 ,size=(4,4))\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6367, -0.1909, -0.4705,  0.7097])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = torch.randn(4)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7690, -1.8104,  0.1360,  0.7058])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn(4)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "Matrix multiplication combines two grids of numbers using systematic rules to create a new grid. Like a recipe that combines ingredients in specific proportions, each element in the result comes from multiplying and adding corresponding elements from both matrices. For example, in image processing, one matrix might represent an image while another represents a transformation to blur or sharpen it.\n",
    "\n",
    "Analytical:\n",
    "Matrix multiplication is a bilinear operation that combines two matrices A ∈ ℝᵐˣⁿ and B ∈ ℝⁿˣᵖ to produce C ∈ ℝᵐˣᵖ. The operation preserves both row and column relationships while combining elements through a series of dot products. The compatibility requires the number of columns in the first matrix to equal the number of rows in the second.\n",
    "\n",
    "Mathematical:\n",
    "For matrices A ∈ ℝᵐˣⁿ and B ∈ ℝⁿˣᵖ, their product C = AB has elements:\n",
    "\n",
    "$$C_{ij} = \\sum_{k=1}^n A_{ik}B_{kj}$$\n",
    "\n",
    "where i = 1,...,m and j = 1,...,p. Each element $C_{ij}$ is the dot product of row i from A with column j from B.\n",
    "\n",
    "Mathematical Notation:\n",
    "A = $[A_{ij}]$ ∈ ℝᵐˣⁿ \n",
    "\n",
    "B = $[B_{ij}]$ ∈ ℝⁿˣᵖ\n",
    "\n",
    "C = AB = $[C_{ij}]$ = $[\\sum_{k=1}^n A_{ik}B_{kj}]$ ∈ ℝᵐˣᵖ\n",
    "\n",
    "Numerical Example:\n",
    "\n",
    "A = $$\\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}$$\n",
    "B = $$\\begin{bmatrix} 1 & 5 \\\\ 2 & 6 \\end{bmatrix}$$\n",
    "\n",
    "Computing C = AB:\n",
    "$C_{11} = (2 × 1) + (3 × 2) = 8$\n",
    "$C_{12} = (2 × 5) + (3 × 6) = 28$\n",
    "$C_{21} = (1 × 1) + (4 × 2) = 9$\n",
    "$C_{22} = (1 × 5) + (4 × 6) = 29$\n",
    "\n",
    "C = $\\begin{bmatrix} 8 & 28 \\\\ 9 & 29 \\end{bmatrix}$\n",
    "\n",
    "Applications:\n",
    "- Linear transformations composition\n",
    "- Computer graphics (3D transformations)\n",
    "- Neural network layers\n",
    "- Markov chains\n",
    "- Data compression\n",
    "- Signal processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 858,  408,  479,  444],\n",
       "        [8580, 4080, 4790, 4440]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "C = torch.mm(A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 858,  408,  479,  444],\n",
       "        [8580, 4080, 4790, 4440]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "C = A @ B                      # Using @ operator\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 858,  408,  479,  444],\n",
       "        [8580, 4080, 4790, 4440]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C =  torch.matmul(A, B)         # General matrix multiplication\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Vector Multiplication\n",
    "\n",
    "User-Friendly Definition:\n",
    "Matrix-vector multiplication is a powerful data transformation technique that works like a customized data processing system. Just as Instagram filters transform photos using preset rules, a matrix acts as a set of mathematical rules that systematically transforms input numbers (vector) into output numbers. Each element in the output is created by combining input numbers in specific ways defined by the matrix. For example, if you have data points representing temperature and humidity, the matrix multiplication could combine them in different proportions to give you a \"comfort index.\" This operation is fundamental in many applications, from image processing to data analysis, where we need to transform data in consistent and meaningful ways.\n",
    "\n",
    "Analytical Definition:\n",
    "Matrix-vector multiplication represents a systematic linear transformation that converts vectors from one space to another while maintaining crucial mathematical properties. It decomposes complex transformations into elementary operations of scaling and combining, encoded within the matrix elements. The transformation preserves the fundamental properties of linearity: additivity and homogeneity. This means that transforming the sum of two vectors equals the sum of their individual transformations, and scaling a vector before transformation is equivalent to scaling after transformation. These properties make matrix-vector multiplication an essential tool in linear algebra, enabling applications from coordinate transformations to solving systems of equations. The operation's power lies in its ability to represent complex linear operations in a compact, computationally efficient form.\n",
    "\n",
    "Mathematical Definition:\n",
    "Matrix-vector multiplication is a precisely defined operation between a matrix A ∈ ℝᵐˣⁿ and a vector x ∈ ℝⁿ that produces a vector b ∈ ℝᵐ. Each component of the resulting vector is computed as the inner product of a row of the matrix with the input vector, following the formula $b_i = \\sum_{j=1}^n A_{ij}x_j$ for i = 1,...,m. This operation can be viewed as a linear transformation from n-dimensional space to m-dimensional space, where each matrix row represents the coefficients of a linear combination. The operation satisfies the distributive property over vector addition and associative property with scalar multiplication, making it a fundamental building block of linear algebra. The dimensionality of the output vector is determined by the number of rows in the matrix, while the input vector's dimension must match the number of matrix columns.\n",
    "\n",
    "Mathematical Notation:\n",
    "Given:\n",
    "A = $[A_{ij}]$ ∈ ℝᵐˣⁿ\n",
    "\n",
    "x = $\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$ ∈ ℝⁿ\n",
    "\n",
    "b = Ax = $\\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix}$ = $\\begin{bmatrix} \\sum_{j=1}^n A_{1j}x_j \\\\ \\sum_{j=1}^n A_{2j}x_j \\\\ \\vdots \\\\ \\sum_{j=1}^n A_{mj}x_j \\end\n",
    "{bmatrix}$ ∈ ℝᵐ\n",
    "\n",
    "Numerical Example:\n",
    "\n",
    "A = $\\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}$\n",
    "\n",
    "x = $\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$\n",
    "\n",
    "Computing b = Ax:\n",
    "$b_1 = (2 \\times 1) + (3 \\times 2) = 8$\n",
    "\n",
    "$b_2 = (1 \\times 1) + (4 \\times 2) = 9$\n",
    "\n",
    "b = $\\begin{bmatrix} 8 \\\\ 9 \\end{bmatrix}$\n",
    "\n",
    "Applications:\n",
    "- Linear transformations (rotation, scaling, shearing)\n",
    "- Feature extraction in ML\n",
    "- Solving linear systems\n",
    "- Neural network layers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me explain matrix-vector multiplication with LaTeX notation and an example.\n",
    "\n",
    "For a matrix A (m×n) and vector x (n×1), their product b = Ax (m×1) is:\n",
    "\n",
    "$b_i = \\sum_{j=1}^n A_{ij}x_j$\n",
    "\n",
    "Example:\n",
    "A = $\\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}$ (2×2)\n",
    "\n",
    "x = $\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$ (2×1)\n",
    "\n",
    "Computing b = Ax:\n",
    "\n",
    "$b_1 = (2 \\times 1) + (3 \\times 2) = 2 + 6 = 8$\n",
    "$b_2 = (1 \\times 1) + (4 \\times 2) = 1 + 8 = 9$\n",
    "\n",
    "Result:\n",
    "b = $\\begin{bmatrix} 8 \\\\ 9 \\end{bmatrix}$\n",
    "\n",
    "In PyTorch, this can be done using `torch.mv(A, x)` or `torch.matmul(A, x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3793, 3.7929])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Vector operations\n",
    "w = torch.mv(A.to(torch.float), W)             # Matrix-vector multiplication\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Dot Product\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "Vector dot product is like calculating a weighted sum, where we multiply corresponding elements from two vectors and add them together. For example, if you have monthly expenses and quantities, dot product gives total cost by multiplying each item's price with its quantity and summing them up.\n",
    "\n",
    "Analytical:\n",
    "The dot product is a fundamental operation that maps two vectors to a scalar, measuring their alignment and magnitude interaction. It's a bilinear form that gives geometric information about vectors' relative orientations and lengths while preserving key algebraic properties.\n",
    "\n",
    "Mathematical:\n",
    "For vectors x, y ∈ ℝⁿ, their dot product is:\n",
    "$x · y = \\sum_{i=1}^n x_iy_i = x_1y_1 + x_2y_2 + ... + x_ny_n$\n",
    "Geometrically: $x · y = ||x|| ||y|| \\cos θ$, where θ is the angle between vectors.\n",
    "\n",
    "Mathematical Notation:\n",
    "x = $\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$\n",
    "y = $\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$\n",
    "x · y = $\\sum_{i=1}^n x_iy_i$ = $x^Ty$ = scalar\n",
    "\n",
    "Numerical Example:\n",
    "x = $\\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}$\n",
    "y = $\\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix}$\n",
    "\n",
    "Computing x · y:\n",
    "= (2 × 1) + (3 × 4)\n",
    "= 2 + 12\n",
    "= 14\n",
    "\n",
    "Applications:\n",
    "- Calculating work in physics (force · displacement)\n",
    "- Finding vector projections\n",
    "- Computing angles between vectors\n",
    "- Neural network layer computations\n",
    "- Similarity measures in ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2928)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = torch.dot(V, W)          # Vector dot product\n",
    "dot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Vector Cross Product\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "Cross product creates a new vector perpendicular to two input vectors, with magnitude related to both vectors' lengths and the sine of angle between them. Like finding a line perpendicular to a plane defined by two vectors.\n",
    "\n",
    "Analytical:\n",
    "A binary operation on three-dimensional vectors that produces a vector orthogonal to both inputs. The result's magnitude represents the area of the parallelogram formed by the vectors, with direction determined by the right-hand rule.\n",
    "\n",
    "Mathematical:\n",
    "For vectors a, b ∈ ℝ³, their cross product a × b is:\n",
    "$(a × b) = ||a|| ||b|| \\sin(θ)\\hat{n}$\n",
    "where θ is angle between vectors and $\\hat{n}$ is unit vector perpendicular to both.\n",
    "\n",
    "Mathematical Notation:\n",
    "a = $\\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{bmatrix}$\n",
    "b = $\\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}$\n",
    "a × b = $\\begin{bmatrix} a_2b_3 - a_3b_2 \\\\ a_3b_1 - a_1b_3 \\\\ a_1b_2 - a_2b_1 \\end{bmatrix}$\n",
    "\n",
    "Numerical Example:\n",
    "a = $\\begin{bmatrix} 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$\n",
    "b = $\\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "Computing a × b:\n",
    "= $\\begin{bmatrix} (3×1) - (4×2) \\\\ (4×1) - (2×1) \\\\ (2×2) - (3×1) \\end{bmatrix}$\n",
    "= $\\begin{bmatrix} -5 \\\\ 2 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "\n",
    "Applications in Data Science & ML:\n",
    "\n",
    "- Feature extraction & dimensionality reduction\n",
    "- Principal Component Analysis (PCA)\n",
    "- Anomaly detection\n",
    "- Pattern recognition in high-dimensional data\n",
    "- Neural network optimizations\n",
    "- Geometric deep learning\n",
    "- Manifold learning\n",
    "- Hyperplane calculations in SVM\n",
    "- Data augmentation\n",
    "- Feature space transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "no dimension of size 3 in input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cross \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m# Vector cross product\u001b[39;00m\n\u001b[1;32m      2\u001b[0m cross\n",
      "\u001b[0;31mRuntimeError\u001b[0m: no dimension of size 3 in input"
     ]
    }
   ],
   "source": [
    "cross = torch.cross(V, V)      # Vector cross product\n",
    "cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer Product\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "Outer product multiplies each element of one vector with every element of another vector, creating a matrix. Like creating a multiplication table between two lists of numbers.\n",
    "\n",
    "Analytical:\n",
    "A tensor product of two vectors that produces a matrix, representing all possible pairwise multiplications between elements of the input vectors. Results in a rank-1 matrix that captures directional information.\n",
    "\n",
    "Mathematical:\n",
    "For vectors x ∈ ℝᵐ and y ∈ ℝⁿ, their outer product is:\n",
    "$(x ⊗ y)_{ij} = x_iy_j$ resulting in matrix A ∈ ℝᵐˣⁿ\n",
    "\n",
    "Mathematical Notation:\n",
    "x = $\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix}$\n",
    "y = $\\begin{bmatrix} y_1 & y_2 & ... & y_n \\end{bmatrix}$\n",
    "x ⊗ y = $\\begin{bmatrix} x_1y_1 & x_1y_2 & ... & x_1y_n \\\\ x_2y_1 & x_2y_2 & ... & x_2y_n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_my_1 & x_my_2 & ... & x_my_n \\end{bmatrix}$\n",
    "\n",
    "Numerical Example:\n",
    "x = $\\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}$\n",
    "y = $\\begin{bmatrix} 1 & 4 \\end{bmatrix}$\n",
    "\n",
    "Result = $\\begin{bmatrix} 2×1 & 2×4 \\\\ 3×1 & 3×4 \\end{bmatrix}$ = $\\begin{bmatrix} 2 & 8 \\\\ 3 & 12 \\end{bmatrix}$\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Attention mechanisms in transformers\n",
    "- Feature interaction modeling\n",
    "- Kernel methods\n",
    "- Embedding space calculations\n",
    "- Covariance matrix estimation\n",
    "- Distance matrix computation\n",
    "- Word embeddings\n",
    "- Recommendation systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4896,  1.1527, -0.0866, -0.4494],\n",
       "        [-0.1468,  0.3455, -0.0260, -0.1347],\n",
       "        [-0.3618,  0.8517, -0.0640, -0.3320],\n",
       "        [ 0.5458, -1.2848,  0.0965,  0.5009]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer = torch.outer(V, W)  \n",
    "outer# Outer product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Hadamard (Element-wise) Product\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "Element-wise product multiplies corresponding elements of two matrices of same size, like comparing item-by-item price changes between two time periods.\n",
    "\n",
    "Analytical:\n",
    "A binary operation that produces a matrix of same dimensions as inputs, where each element is product of corresponding elements from input matrices, preserving element-wise relationships.\n",
    "\n",
    "Mathematical:\n",
    "For matrices A, B ∈ ℝᵐˣⁿ, their Hadamard product is:\n",
    "(A ⊙ B)ᵢⱼ = Aᵢⱼ × Bᵢⱼ\n",
    "\n",
    "Mathematical Notation:\n",
    "A = [aᵢⱼ], B = [bᵢⱼ] ∈ ℝᵐˣⁿ\n",
    "A ⊙ B = [aᵢⱼ × bᵢⱼ] ∈ ℝᵐˣⁿ\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}$\n",
    "B = $\\begin{bmatrix} 1 & 2 \\\\ 3 & 1 \\end{bmatrix}$\n",
    "\n",
    "A ⊙ B = $\\begin{bmatrix} 2×1 & 3×2 \\\\ 1×3 & 4×1 \\end{bmatrix}$ = $\\begin{bmatrix} 2 & 6 \\\\ 3 & 4 \\end{bmatrix}$\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Neural network gates (LSTM, GRU)\n",
    "- Attention mechanisms\n",
    "- Feature interactions\n",
    "- Gradient computations\n",
    "- Mask operations\n",
    "- Element-wise weighting\n",
    "- Signal processing\n",
    "- Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Matrix Addition and Subtraction\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "Matrix addition/subtraction combines/reduces corresponding elements of same-sized matrices, like combining/comparing expenses across different departments in monthly budgets.\n",
    "\n",
    "Analytical:\n",
    "Binary operations on matrices of identical dimensions, preserving element-wise operations while maintaining distributive and associative properties.\n",
    "\n",
    "Mathematical:\n",
    "For matrices A, B ∈ ℝᵐˣⁿ, their sum/difference is:\n",
    "(A ± B)ᵢⱼ = Aᵢⱼ ± Bᵢⱼ\n",
    "\n",
    "Mathematical Notation:\n",
    "A = [aᵢⱼ], B = [bᵢⱼ] ∈ ℝᵐˣⁿ\n",
    "A ± B = [aᵢⱼ ± bᵢⱼ] ∈ ℝᵐˣⁿ\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}$\n",
    "B = $\\begin{bmatrix} 1 & 2 \\\\ 3 & 1 \\end{bmatrix}$\n",
    "\n",
    "Addition:\n",
    "A + B = $\\begin{bmatrix} 3 & 5 \\\\ 4 & 5 \\end{bmatrix}$\n",
    "\n",
    "Subtraction:\n",
    "A - B = $\\begin{bmatrix} 1 & 1 \\\\ -2 & 3 \\end{bmatrix}$\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Residual networks\n",
    "- Gradient operations\n",
    "- Feature engineering\n",
    "- State updates in RNNs\n",
    "- Error computation\n",
    "- Layer normalization\n",
    "- Feature differencing\n",
    "- Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Element-wise operations\n",
    "hadamard = A * B               # Hadamard (element-wise) product\n",
    "sum_matrices = A + B           # Matrix addition\n",
    "diff_matrices = A - B          # Matrix subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Matrix Determinant\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "The determinant is a scalar value calculated from a square matrix that indicates important matrix properties like invertibility and scaling factor for area/volume transformation.\n",
    "\n",
    "Analytical:\n",
    "A scalar function that maps square matrices to real numbers, providing crucial information about linear transformations including orientation, scaling, and singularity.\n",
    "\n",
    "Mathematical:\n",
    "For a 2×2 matrix A = $\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$\n",
    "det(A) = ad - bc\n",
    "\n",
    "For n×n matrices, recursively calculated using cofactor expansion:\n",
    "det(A) = $\\sum_{j=1}^n a_{1j}(-1)^{1+j}M_{1j}$\n",
    "\n",
    "Mathematical Notation:\n",
    "For matrix A:\n",
    "det(A) or |A|\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}$\n",
    "\n",
    "det(A) = (2 × 4) - (3 × 1)\n",
    "= 8 - 3\n",
    "= 5\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Feature importance assessment\n",
    "- Principal Component Analysis\n",
    "- Matrix invertibility checking\n",
    "- Volume calculations\n",
    "- Multivariate distributions\n",
    "- Change of variables\n",
    "- Eigenvalue analysis\n",
    "- Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Matrix Rank\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "Matrix rank represents the number of linearly independent rows or columns, indicating how much unique information the matrix contains.\n",
    "\n",
    "Analytical:\n",
    "A measure of the dimensional span of a matrix's row/column space, equal to the number of non-zero singular values or dimension of matrix's range.\n",
    "\n",
    "Mathematical:\n",
    "For matrix A ∈ ℝᵐˣⁿ, rank(A) is:\n",
    "- Number of linearly independent rows/columns\n",
    "- Dimension of column/row space\n",
    "- Number of non-zero singular values\n",
    "\n",
    "Mathematical Notation:\n",
    "rank(A) = r ≤ min(m,n)\n",
    "where r is number of linearly independent rows/columns\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\\\ 3 & 5 & 7 \\end{bmatrix}$\n",
    "\n",
    "rank(A) = 2 because:\n",
    "- Row 2 = 2 × Row 1 \n",
    "- All rows are linear combinations of Rows 1 and 3\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Dimensionality reduction\n",
    "- Feature selection\n",
    "- Matrix factorization\n",
    "- Compression techniques\n",
    "- Low-rank approximation\n",
    "- Principal Component Analysis\n",
    "- Linear dependency analysis\n",
    "- Subspace learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Matrix Properties =====\n",
    "# Determinant and rank\n",
    "det = torch.det(A)             # Matrix determinant\n",
    "det = LA.det(A)               # Alternative using torch.linalg\n",
    "rank = LA.matrix_rank(A)      # Matrix rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Matrix Trace\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "The trace is the sum of elements on main diagonal (top-left to bottom-right), representing a quick way to measure total self-interaction in a square matrix.\n",
    "\n",
    "Analytical:\n",
    "A linear operator mapping square matrices to scalars, equal to sum of eigenvalues and invariant under similarity transformations.\n",
    "\n",
    "Mathematical:\n",
    "For matrix A ∈ ℝⁿˣⁿ:\n",
    "tr(A) = $\\sum_{i=1}^n a_{ii}$ = $\\sum_{i=1}^n \\lambda_i$\n",
    "where λᵢ are eigenvalues\n",
    "\n",
    "Mathematical Notation:\n",
    "tr(A) or $\\sum_{i=1}^n a_{ii}$\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}$\n",
    "\n",
    "tr(A) = 2 + 4 = 6\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Loss function computation\n",
    "- Network architecture optimization\n",
    "- Kernel methods\n",
    "- Covariance analysis\n",
    "- Model regularization\n",
    "- Dimensionality reduction\n",
    "- Feature selection\n",
    "- Gradient calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Eigenvalues\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "Eigenvalues are special scaling factors that tell us how a matrix stretches or compresses vectors in particular directions.\n",
    "\n",
    "Analytical:\n",
    "Scalars that characterize linear transformation's action on special vectors (eigenvectors), representing invariant scaling factors under transformation.\n",
    "\n",
    "Mathematical:\n",
    "For matrix A ∈ ℝⁿˣⁿ, λ is an eigenvalue if:\n",
    "Av = λv\n",
    "for some non-zero vector v (eigenvector)\n",
    "det(A - λI) = 0\n",
    "\n",
    "Mathematical Notation:\n",
    "Characteristic equation:\n",
    "det(A - λI) = 0\n",
    "where I is identity matrix\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$\n",
    "\n",
    "det(A - λI) = $\\begin{vmatrix} 2-λ & 1 \\\\ 1 & 2-λ \\end{vmatrix}$ = 0\n",
    "(2-λ)(2-λ) - 1 = 0\n",
    "λ = 3 or 1\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Principal Component Analysis\n",
    "- Dimensionality reduction\n",
    "- Spectral clustering\n",
    "- Covariance analysis\n",
    "- Feature importance\n",
    "- Graph analysis\n",
    "- Stability analysis\n",
    "- Network embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Similarity Transformations\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "A transformation that preserves matrix's essential properties (eigenvalues) while changing its representation, like viewing same object from different angles.\n",
    "\n",
    "Analytical:\n",
    "A transformation of form B = P⁻¹AP where P is invertible, preserving eigenvalues and other spectral properties while potentially simplifying matrix structure.\n",
    "\n",
    "Mathematical:\n",
    "For matrix A, similarity transformation produces B:\n",
    "B = P⁻¹AP\n",
    "where P is invertible\n",
    "\n",
    "Mathematical Notation:\n",
    "A ~ B ⟺ ∃P: B = P⁻¹AP\n",
    "where ~ denotes \"similar to\"\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$\n",
    "P = $\\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix}$\n",
    "\n",
    "B = P⁻¹AP = $\\begin{bmatrix} 3 & 0 \\\\ 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Diagonalization\n",
    "- Feature transformation\n",
    "- Basis change\n",
    "- Dimensionality reduction\n",
    "- Spectral methods\n",
    "- Matrix decomposition\n",
    "- Kernel methods\n",
    "- Pattern recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Trace and diagonal\n",
    "tr = torch.trace(A)            # Matrix trace\n",
    "diag = torch.diag(A)           # Extract diagonal\n",
    "diag_mat = torch.diag(v)       # Create diagonal matrix from vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Frobenius Norm\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "A measure of matrix size that considers all elements, calculated by taking square root of sum of squared elements, like measuring total energy in a system.\n",
    "\n",
    "Analytical:\n",
    "Matrix norm that treats matrix as vector of its elements, providing Euclidean-like measure of matrix magnitude while being computationally tractable.\n",
    "\n",
    "Mathematical:\n",
    "For matrix A ∈ ℝᵐˣⁿ:\n",
    "||A||ᶠ = $\\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2}$ = $\\sqrt{tr(A^TA)}$\n",
    "\n",
    "Mathematical Notation:\n",
    "||A||ᶠ or ||A||₂\n",
    "= $\\sqrt{\\sum_{i,j} |a_{ij}|^2}$\n",
    "= $\\sqrt{\\sum_{i=1}^r σᵢ²}$ (using singular values)\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}$\n",
    "\n",
    "||A||ᶠ = $\\sqrt{2² + 3² + 1² + 4²}$\n",
    "= $\\sqrt{4 + 9 + 1 + 16}$\n",
    "= $\\sqrt{30}$ ≈ 5.48\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Loss function design\n",
    "- Model regularization\n",
    "- Matrix approximation\n",
    "- Error measurement\n",
    "- Weight initialization\n",
    "- Gradient clipping\n",
    "- Model comparison\n",
    "- Convergence analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Nuclear Norm\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "Nuclear norm is sum of matrix's singular values, providing measure of matrix's \"complexity\" or \"effective rank\", useful in finding simple yet effective solutions.\n",
    "\n",
    "Analytical:\n",
    "A matrix norm equal to sum of singular values, serving as convex approximation of matrix rank and promoting low-rank solutions in optimization problems.\n",
    "\n",
    "Mathematical:\n",
    "For matrix A ∈ ℝᵐˣⁿ:\n",
    "||A||* = $\\sum_{i=1}^r σᵢ$\n",
    "where σᵢ are singular values and r = rank(A)\n",
    "= tr($\\sqrt{A^TA}$)\n",
    "\n",
    "Mathematical Notation:\n",
    "||A||* = $\\sum_{i=1}^r σᵢ$\n",
    "where σᵢ are singular values\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}$\n",
    "\n",
    "Singular values: σ₁ ≈ 5.47, σ₂ ≈ 0.37\n",
    "||A||* = 5.47 + 0.37 ≈ 5.84\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Matrix completion\n",
    "- Recommendation systems\n",
    "- Dimensionality reduction\n",
    "- Low-rank approximation\n",
    "- Feature selection\n",
    "- Collaborative filtering\n",
    "- Robust PCA\n",
    "- Signal recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Spectral Norm\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "Spectral norm represents maximum \"stretching\" factor of matrix transformation, measuring largest possible amplification when matrix acts on a unit vector.\n",
    "\n",
    "Analytical:\n",
    "Maximum singular value of matrix, equivalent to square root of largest eigenvalue of A^TA, representing operator's maximum gain.\n",
    "\n",
    "Mathematical:\n",
    "For matrix A ∈ ℝᵐˣⁿ:\n",
    "||A||₂ = $\\sqrt{λ_{max}(A^TA)}$ = σ₁\n",
    "= max{||Ax||₂ : ||x||₂ = 1}\n",
    "where σ₁ is largest singular value\n",
    "\n",
    "Mathematical Notation:\n",
    "||A||₂ or σ₁(A)\n",
    "= max{||Ax||₂/||x||₂ : x ≠ 0}\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}$\n",
    "\n",
    "Singular values: σ₁ ≈ 5.47, σ₂ ≈ 0.37\n",
    "||A||₂ = σ₁ ≈ 5.47\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Neural network stability\n",
    "- Gradient clipping\n",
    "- Model regularization\n",
    "- Lipschitz continuity\n",
    "- Robustness analysis\n",
    "- Weight normalization\n",
    "- Convergence analysis\n",
    "- Network architecture design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Vector Norm\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "A measure of vector's \"size\" or \"length\", like measuring distance or magnitude. Different norms measure size in different ways, similar to measuring distance using different paths.\n",
    "\n",
    "Analytical:\n",
    "A function mapping vectors to non-negative real numbers, satisfying properties of non-negativity, homogeneity, and triangle inequality.\n",
    "\n",
    "Mathematical:\n",
    "For vector x ∈ ℝⁿ:\n",
    "L₁ norm: ||x||₁ = $\\sum_{i=1}^n |x_i|$\n",
    "L₂ norm: ||x||₂ = $\\sqrt{\\sum_{i=1}^n x_i^2}$\n",
    "L∞ norm: ||x||∞ = max|xᵢ|\n",
    "\n",
    "Mathematical Notation:\n",
    "General p-norm:\n",
    "||x||ₚ = $(\\sum_{i=1}^n |x_i|^p)^{1/p}$\n",
    "\n",
    "Numerical Example:\n",
    "x = $\\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}$\n",
    "\n",
    "L₁ norm: |3| + |4| = 7\n",
    "L₂ norm: $\\sqrt{3² + 4²}$ = 5\n",
    "L∞ norm: max(|3|, |4|) = 4\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Feature scaling\n",
    "- Distance metrics\n",
    "- Loss functions\n",
    "- Regularization\n",
    "- Outlier detection\n",
    "- Gradient clipping\n",
    "- Robustness analysis\n",
    "- Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Norms\n",
    "norm_frobenius = LA.norm(A, 'fro')    # Frobenius norm\n",
    "norm_nuclear = LA.norm(A, 'nuc')      # Nuclear norm\n",
    "norm_spectral = LA.norm(A, 2)         # Spectral norm (largest singular value)\n",
    "norm_vector = LA.vector_norm(v)       # Vector norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Matrix Decomposition\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "Breaking down a complex matrix into simpler component matrices, like factoring a number into primes. Each decomposition reveals different aspects of original matrix.\n",
    "\n",
    "Analytical:\n",
    "Factorization of a matrix into a product of matrices with special properties, enabling efficient computation and revealing underlying structure of linear transformations.\n",
    "\n",
    "Mathematical:\n",
    "Common decompositions:\n",
    "- SVD: A = UΣV^T\n",
    "- LU: A = LU\n",
    "- QR: A = QR\n",
    "- Eigendecomposition: A = PDP⁻¹ (if A diagonalizable)\n",
    "\n",
    "Mathematical Notation:\n",
    "SVD: A = UΣV^T where\n",
    "U: orthogonal left singular vectors\n",
    "Σ: diagonal singular values\n",
    "V^T: orthogonal right singular vectors\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 4 & 0 \\\\ 3 & -5 \\end{bmatrix}$\n",
    "\n",
    "LU Decomposition:\n",
    "L = $\\begin{bmatrix} 1 & 0 \\\\ 3/4 & 1 \\end{bmatrix}$\n",
    "U = $\\begin{bmatrix} 4 & 0 \\\\ 0 & -5 \\end{bmatrix}$\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Dimensionality reduction (PCA)\n",
    "- Feature extraction\n",
    "- Data compression\n",
    "- Recommendation systems\n",
    "- Image processing\n",
    "- Noise reduction\n",
    "- Matrix completion\n",
    "- Spectral clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Singular Value Decomposition (SVD)\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "SVD breaks down any matrix into three components: directions of maximum variation (U), strength of these variations (Σ), and transformed directions (V^T), like decomposing complex motion into principal movements.\n",
    "\n",
    "Analytical:\n",
    "A fundamental matrix factorization that decomposes matrix into product of orthogonal and diagonal matrices, revealing geometric structure of linear transformation.\n",
    "\n",
    "Mathematical:\n",
    "For matrix A ∈ ℝᵐˣⁿ:\n",
    "A = UΣV^T where\n",
    "- U ∈ ℝᵐˣᵐ (orthogonal)\n",
    "- Σ ∈ ℝᵐˣⁿ (diagonal)\n",
    "- V^T ∈ ℝⁿˣⁿ (orthogonal)\n",
    "\n",
    "Mathematical Notation:\n",
    "A = UΣV^T\n",
    "where σᵢ (diagonal elements of Σ) are singular values\n",
    "U: left singular vectors\n",
    "V: right singular vectors\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 4 & 0 \\\\ 3 & -5 \\end{bmatrix}$\n",
    "\n",
    "SVD:\n",
    "U ≈ $\\begin{bmatrix} 0.78 & -0.62 \\\\ 0.62 & 0.78 \\end{bmatrix}$\n",
    "Σ ≈ $\\begin{bmatrix} 6.71 & 0 \\\\ 0 & 3.46 \\end{bmatrix}$\n",
    "V^T ≈ $\\begin{bmatrix} 0.60 & -0.80 \\\\ -0.80 & -0.60 \\end{bmatrix}$\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Principal Component Analysis\n",
    "- Data compression\n",
    "- Dimensionality reduction\n",
    "- Matrix approximation\n",
    "- Recommendation systems\n",
    "- Image processing\n",
    "- Feature extraction\n",
    "- Noise reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Matrix Decompositions =====\n",
    "# Singular Value Decomposition (SVD)\n",
    "U, S, Vh = LA.svd(A)           # Full SVD\n",
    "U, S, Vh = LA.svd(A, full_matrices=False)  # Economy SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Eigendecomposition\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "Factorizes a square matrix into eigenvectors (directions of pure scaling/rotation) and eigenvalues (amount of scaling), revealing fundamental behavior of linear transformations.\n",
    "\n",
    "Analytical:\n",
    "A decomposition of square matrix expressing it as product of eigenvectors and diagonal matrix of eigenvalues, applicable when matrix has complete set of eigenvectors.\n",
    "\n",
    "Mathematical:\n",
    "For diagonalizable matrix A ∈ ℝⁿˣⁿ:\n",
    "A = PDP⁻¹ where\n",
    "- P: eigenvector matrix\n",
    "- D: diagonal matrix of eigenvalues\n",
    "- P⁻¹: inverse of eigenvector matrix\n",
    "\n",
    "Mathematical Notation:\n",
    "A = PDP⁻¹\n",
    "where D = diag(λ₁,...,λₙ)\n",
    "and P = [v₁|v₂|...|vₙ]\n",
    "λᵢ: eigenvalues\n",
    "vᵢ: eigenvectors\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$\n",
    "\n",
    "Eigendecomposition:\n",
    "P = $\\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix}$\n",
    "D = $\\begin{bmatrix} 3 & 0 \\\\ 0 & 1 \\end{bmatrix}$\n",
    "P⁻¹ = $\\begin{bmatrix} 0.5 & 0.5 \\\\ 0.5 & -0.5 \\end{bmatrix}$\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Principal Component Analysis\n",
    "- Spectral clustering\n",
    "- Covariance analysis\n",
    "- Dimensionality reduction\n",
    "- Feature transformation\n",
    "- Signal processing\n",
    "- Graph analysis\n",
    "- Dynamic systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Hermitian/Symmetric Eigendecomposition\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "Special decomposition for symmetric matrices where eigenvalues are real and eigenvectors are orthogonal, simplifying many computations like finding principal directions in data.\n",
    "\n",
    "Analytical:\n",
    "Eigendecomposition of Hermitian/symmetric matrices featuring real eigenvalues and orthogonal eigenvectors, guaranteeing diagonalization with orthogonal matrix.\n",
    "\n",
    "Mathematical:\n",
    "For symmetric matrix A ∈ ℝⁿˣⁿ:\n",
    "A = QΛQ^T where\n",
    "- Q: orthogonal matrix of eigenvectors\n",
    "- Λ: diagonal matrix of real eigenvalues\n",
    "- Q^T = Q⁻¹ (orthogonality)\n",
    "\n",
    "Mathematical Notation:\n",
    "A = QΛQ^T\n",
    "where Λ = diag(λ₁,...,λₙ)\n",
    "Q = [q₁|q₂|...|qₙ]\n",
    "⟨qᵢ,qⱼ⟩ = δᵢⱼ (orthonormality)\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$\n",
    "\n",
    "Q = $\\begin{bmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\ 1/\\sqrt{2} & -1/\\sqrt{2} \\end{bmatrix}$\n",
    "Λ = $\\begin{bmatrix} 3 & 0 \\\\ 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Covariance matrix analysis\n",
    "- Principal Component Analysis\n",
    "- Kernel methods\n",
    "- Spectral clustering\n",
    "- Dimensionality reduction\n",
    "- Signal processing\n",
    "- Feature extraction\n",
    "- Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Eigendecomposition\n",
    "eigenvals, eigenvecs = LA.eig(A)      # Eigendecomposition\n",
    "eigenvals, eigenvecs = LA.eigh(A)     # Hermitian/symmetric eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: LU Decomposition\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "Breaks matrix into product of lower triangular (L) and upper triangular (U) matrices, like solving equations step by step, making complex calculations simpler.\n",
    "\n",
    "Analytical:\n",
    "Factorization of matrix into lower and upper triangular matrices, useful for solving linear systems efficiently and avoiding repeated eliminations.\n",
    "\n",
    "Mathematical:\n",
    "For matrix A ∈ ℝⁿˣⁿ:\n",
    "A = LU where\n",
    "- L: lower triangular (lᵢⱼ = 0 for i < j)\n",
    "- U: upper triangular (uᵢⱼ = 0 for i > j)\n",
    "- L typically has diagonal of 1s\n",
    "\n",
    "Mathematical Notation:\n",
    "A = LU\n",
    "L = [lᵢⱼ], lᵢⱼ = 0 for i < j\n",
    "U = [uᵢⱼ], uᵢⱼ = 0 for i > j\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 4 & 3 \\\\ 6 & 3 \\end{bmatrix}$\n",
    "\n",
    "L = $\\begin{bmatrix} 1 & 0 \\\\ 3/2 & 1 \\end{bmatrix}$\n",
    "U = $\\begin{bmatrix} 4 & 3 \\\\ 0 & -1.5 \\end{bmatrix}$\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Linear system solving\n",
    "- Matrix inversion\n",
    "- Determinant calculation\n",
    "- Numerical stability analysis\n",
    "- Network analysis\n",
    "- Time series modeling\n",
    "- System identification\n",
    "- Pattern recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LU Decomposition\n",
    "LU, pivots = LA.lu_factor(A)          # LU factorization\n",
    "P, L, U = LA.lu(A)                    # Complete LU decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: QR Decomposition\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "Decomposes matrix into orthogonal matrix (Q) and upper triangular matrix (R), like breaking movement into perpendicular directions and their magnitudes.\n",
    "\n",
    "Analytical:\n",
    "Factorization expressing matrix as product of orthogonal matrix and upper triangular matrix, useful for solving least squares and finding orthonormal bases.\n",
    "\n",
    "Mathematical:\n",
    "For matrix A ∈ ℝᵐˣⁿ:\n",
    "A = QR where\n",
    "- Q: orthogonal matrix (Q^TQ = I)\n",
    "- R: upper triangular matrix\n",
    "- Q^T = Q⁻¹ (orthogonality)\n",
    "\n",
    "Mathematical Notation:\n",
    "A = QR\n",
    "Q ∈ ℝᵐˣᵐ (orthogonal)\n",
    "R ∈ ℝᵐˣⁿ (upper triangular)\n",
    "Q^TQ = I\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 1 & 1 \\\\ 1 & 0 \\end{bmatrix}$\n",
    "\n",
    "Q ≈ $\\begin{bmatrix} 0.707 & 0.707 \\\\ 0.707 & -0.707 \\end{bmatrix}$\n",
    "R ≈ $\\begin{bmatrix} 1.414 & 0.707 \\\\ 0 & 0.707 \\end{bmatrix}$\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Least squares problems\n",
    "- Linear regression\n",
    "- Feature orthogonalization\n",
    "- Eigenvalue algorithms\n",
    "- Signal processing\n",
    "- Matrix factorization\n",
    "- Subspace tracking\n",
    "- Optimization problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# QR Decomposition\n",
    "Q, R = LA.qr(A)                       # QR decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Cholesky Decomposition\n",
    "\n",
    "Definition:\n",
    "\n",
    "User-Friendly:\n",
    "Special decomposition for symmetric positive-definite matrices into product of lower triangular matrix and its transpose, like finding \"square root\" of matrix.\n",
    "\n",
    "Analytical:\n",
    "Unique factorization of symmetric positive-definite matrix into product of lower triangular matrix and its transpose, computationally efficient and numerically stable.\n",
    "\n",
    "Mathematical:\n",
    "For symmetric positive-definite A ∈ ℝⁿˣⁿ:\n",
    "A = LL^T where\n",
    "- L: lower triangular matrix\n",
    "- L^T: transpose of L (upper triangular)\n",
    "- A must be symmetric and positive definite\n",
    "\n",
    "Mathematical Notation:\n",
    "A = LL^T\n",
    "L ∈ ℝⁿˣⁿ (lower triangular)\n",
    "A = A^T and x^TAx > 0 for x ≠ 0\n",
    "\n",
    "Numerical Example:\n",
    "A = $\\begin{bmatrix} 4 & 2 \\\\ 2 & 3 \\end{bmatrix}$\n",
    "\n",
    "L = $\\begin{bmatrix} 2 & 0 \\\\ 1 & \\sqrt{2} \\end{bmatrix}$\n",
    "L^T = $\\begin{bmatrix} 2 & 1 \\\\ 0 & \\sqrt{2} \\end{bmatrix}$\n",
    "\n",
    "Applications in ML & Data Science:\n",
    "- Covariance matrix analysis\n",
    "- Monte Carlo simulation\n",
    "- Linear regression\n",
    "- Optimization algorithms\n",
    "- Gaussian processes\n",
    "- Mahalanobis distance\n",
    "- Kalman filtering\n",
    "- Portfolio optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cholesky Decomposition\n",
    "L = LA.cholesky(A @ A.T)              # Cholesky decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== System Solving =====\n",
    "# Solve linear system Ax = b\n",
    "b = torch.randn(3)\n",
    "x = LA.solve(A, b)                    # Solve Ax = b\n",
    "x = LA.lstsq(A, b)                    # Least squares solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Matrix inverse\n",
    "inv_A = LA.inv(A)                     # Matrix inverse\n",
    "pinv_A = LA.pinv(A)                   # Pseudoinverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Advanced Operations =====\n",
    "# Matrix functions\n",
    "exp_A = LA.matrix_exp(A)              # Matrix exponential\n",
    "power_A = LA.matrix_power(A, 3)       # Matrix power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sudhamshuaddanki/smaddanki/Repos/d2l/.venv/lib/python3.12/site-packages/torch/functional.py:2033: UserWarning: torch.chain_matmul is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot instead, which accepts a list of two or more tensors rather than multiple parameters. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1109.)\n",
      "  return _VF.chain_matmul(matrices)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Matrix products\n",
    "multi_mm = torch.chain_matmul(A, B, A)  # Chain matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triangular operations\n",
    "upper_tri = torch.triu(A)             # Extract upper triangular\n",
    "lower_tri = torch.tril(A)             # Extract lower triangular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Additional Properties =====\n",
    "def matrix_properties(A):\n",
    "    \"\"\"Compute various matrix properties\"\"\"\n",
    "    props = {\n",
    "        'is_symmetric': torch.allclose(A, A.T),\n",
    "        'is_positive_definite': is_positive_definite(A),\n",
    "        'condition_number': LA.cond(A),\n",
    "        'rank': LA.matrix_rank(A),\n",
    "        'determinant': LA.det(A),\n",
    "        'trace': torch.trace(A)\n",
    "    }\n",
    "    return props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_positive_definite(A):\n",
    "    \"\"\"Check if matrix is positive definite\"\"\"\n",
    "    try:\n",
    "        LA.cholesky(A)\n",
    "        return True\n",
    "    except RuntimeError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Special Matrices =====\n",
    "def create_special_matrices(n):\n",
    "    \"\"\"Create various special matrices\"\"\"\n",
    "    matrices = {\n",
    "        'identity': torch.eye(n),\n",
    "        'zeros': torch.zeros(n, n),\n",
    "        'ones': torch.ones(n, n),\n",
    "        'random': torch.randn(n, n),\n",
    "        'orthogonal': LA.qr(torch.randn(n, n))[0],\n",
    "        'symmetric': lambda x: (x + x.T)/2,\n",
    "        'diagonal': torch.diag(torch.randn(n))\n",
    "    }\n",
    "    return matrices\n",
    "\n",
    "# ===== Performance Optimized Operations =====\n",
    "# Batch operations\n",
    "batch_A = torch.randn(10, 3, 3)  # Batch of matrices\n",
    "batch_b = torch.randn(10, 3)     # Batch of vectors\n",
    "\n",
    "# Batch matrix multiplication\n",
    "batch_C = torch.bmm(batch_A, batch_A.transpose(1, 2))  # Batch matrix-matrix product\n",
    "batch_x = LA.solve(batch_A, batch_b.unsqueeze(-1))     # Batch solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
